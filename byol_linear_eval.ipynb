{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-09T04:45:19.546664Z","iopub.status.busy":"2023-05-09T04:45:19.546261Z","iopub.status.idle":"2023-05-09T04:45:45.259269Z","shell.execute_reply":"2023-05-09T04:45:45.258176Z","shell.execute_reply.started":"2023-05-09T04:45:19.546629Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting imutils\n","  Downloading imutils-0.5.4.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: imutils\n","  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25859 sha256=078b49ba544eade09bd4f1c033bd2322c764e5a53cb881227a464541874538bf\n","  Stored in directory: /root/.cache/pip/wheels/85/cf/3a/e265e975a1e7c7e54eb3692d6aa4e2e7d6a3945d29da46f2d7\n","Successfully built imutils\n","Installing collected packages: imutils\n","Successfully installed imutils-0.5.4\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting gdown\n","  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.64.1)\n","Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.28.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.11.0)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","Installing collected packages: gdown\n","Successfully installed gdown-4.7.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mDownloading...\n","From (uriginal): https://drive.google.com/uc?id=1pGDcwitiFXy-CY9vO3wH3XprwjWgsQ8R\n","From (redirected): https://drive.google.com/uc?id=1pGDcwitiFXy-CY9vO3wH3XprwjWgsQ8R&confirm=t&uuid=22df4aad-7338-419d-a078-e35f46a7b822\n","To: /kaggle/working/semi_super_augPipe.py\n","100%|██████████████████████████████████████| 12.2k/12.2k [00:00<00:00, 43.5MB/s]\n"]}],"source":["!pip install imutils \n","! pip install gdown \n","! gdown https://drive.google.com/uc?id=1pGDcwitiFXy-CY9vO3wH3XprwjWgsQ8R"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:59:57.692764Z","iopub.status.busy":"2023-05-09T10:59:57.692257Z","iopub.status.idle":"2023-05-09T11:00:07.401446Z","shell.execute_reply":"2023-05-09T11:00:07.400469Z","shell.execute_reply.started":"2023-05-09T10:59:57.692715Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["from tensorflow import keras \n","import tensorflow as tf \n","from tensorflow.keras import models \n","import os \n","#from imutils import paths \n","import matplotlib.pyplot as plt \n","import numpy as np \n","\n","import shutil\n","import cv2\n","import random\n","from dataclasses import dataclass \n","from tqdm import tqdm\n","import tempfile\n","from semi_super_augPipe import preprocess_image\n","from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPooling2D, GlobalMaxPooling2D, \\\n","                                                GlobalAveragePooling2D, BatchNormalization, Flatten, ReLU"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T11:00:38.442405Z","iopub.status.busy":"2023-05-09T11:00:38.442051Z","iopub.status.idle":"2023-05-09T11:00:38.448850Z","shell.execute_reply":"2023-05-09T11:00:38.447951Z","shell.execute_reply.started":"2023-05-09T11:00:38.442376Z"},"trusted":true},"outputs":[],"source":["L2_PENALTY = 1.5*10e-6"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T11:00:07.403670Z","iopub.status.busy":"2023-05-09T11:00:07.403322Z","iopub.status.idle":"2023-05-09T11:00:07.429486Z","shell.execute_reply":"2023-05-09T11:00:07.428627Z","shell.execute_reply.started":"2023-05-09T11:00:07.403638Z"},"trusted":true},"outputs":[],"source":["class DataLoader: \n","    \"\"\"\n","        Class, will be useful for creating the BYOL dataset or dataset for the DownStream task \n","            like classification or segmentation.\n","        Methods:\n","            __download_data(scope: private)\n","            __normalize(scope: private)\n","            __preprocess_img(scope: private)\n","             __get_valdata(scope: private)\n","            get_byol_dataset(scope: public)\n","            get_downstream_data(scope: public)\n","        \n","        Property:\n","            dname(dtype: str)        : dataset name(supports cifar10, cifar100).\n","            byol_augmentor(type      : ByolAugmentor): byol augmentor instance/object.\n","            nval(type: int)          : Number of validation data needed, this will be created by splitting the testing\n","                                       data.\n","            resize_shape(dtype: int) : Resize shape, bcoz pretrained models, might have a different required shape.\n","            normalize(dtype: bool)   : bool value, whether to normalize the data or not. \n","    \"\"\"\n","    \n","    def __init__(self, dname=\"cifar10\", byol_augmentor=None, nval=5000,\n","                                             resize_shape=96, normalize=True, downstream_data=False): \n","        assert (byol_augmentor != None or downstream_data), 'Need a BYOL Augment object'\n","        assert dname in [\"cifar10\", 'cifar100'], \"dname should be either cifar10 or cifar100\"\n","        assert nval <= 10_000, \"ValueError: nval value should be <= 10_000\"\n","        \n","        __train_data, __test_data = self.__download_data(dname)\n","        self.__train_X, self.__train_y = __train_data\n","        self.__train_X, self.__train_y = self.__train_X[: 20000], self.__train_y[: 20000]\n","      #  self.__train_X, self.__train_y = self.__train_X[: 100], self.__train_y[: 100]\n","        self.__dtest_X, self.__dtest_y = __test_data \n","        self.class_name = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n","                                           'dog', 'frog', 'horse', 'sheep', 'truck']\n","        self.byol_augmentor = byol_augmentor\n","        self.__get_valdata(nval)\n","        self.resize_shape = resize_shape\n","        \n","        self.__normalize() if normalize else None\n","        self.min_obj_cov_value = 0.7\n","        self.color_jitter_value = 0.1\n","        \n","    def __len__(self): \n","        return self.__train_X.shape[0] + self.__dtest_X.shape[0]\n","    \n","    def __repr__(self): \n","        return f\"Training Samples: {self.__train_X.shape[0]}, Testing Samples: {self.__dtest_X.shape[0]}\"\n","    \n","    def __download_data(self, dname):\n","        \"\"\"\n","            Downloads the data from the tensorflow website using the tensorflw.keras.load_data() method.\n","            Params:\n","                dname(type: Str): dataset name, it just supports two dataset cifar10 or cifar100\n","            Return(type(np.ndarray, np.ndarray))\n","                returns the training data and testing data\n","        \"\"\"\n","        if dname == \"cifar10\": \n","            train_data, test_data = tf.keras.datasets.cifar10.load_data()\n","        if dname == \"cifar100\": \n","            train_data, test_data = tf.keras.datasets.cifar100.load_data()\n","            \n","        return train_data, test_data\n","    \n","    def __normalize(self): \n","        \"\"\"\n","            this method, will used to normalize the inputs.\n","        \"\"\"\n","        self.__train_X = self.__train_X / 255.0\n","        self.__dtest_X = self.__dtest_X / 255.0\n","    \n","    def __preprocess_img(self, image): \n","        \"\"\"\n","            this method, will be used by the get_byol_dataset methos, which does a convertion of \n","            numpy data to tensorflow data.\n","            Params:\n","                image(type: np.ndarray): image data.\n","            Returns(type; (np.ndarray, np.ndarray))\n","                returns the two different augmented views of same image.\n","        \"\"\"\n","        try: \n","            image = tf.image.convert_image_dtype(image, tf.float32)\n","            image = tf.image.resize(image, (self.resize_shape, self.resize_shape))\n","           # view1 = self.byol_augmentor.augment(image, self.resize_shape)\n","            #view2 = self.byol_augmentor.augment(image, self.resize_shape)\n","            \n","            view1 = preprocess_image(image = image, \n","                                   height = 96, \n","                                   width  = 96, \n","                                   cjs = self.color_jitter_value,\n","                                   m_obj_cov = self.min_obj_cov_value,\n","                                   a_range = (self.min_obj_cov_value, 1.0))\n","            \n","            view2 = preprocess_image(image = image, \n","                                   height = 96, \n","                                   width  = 96, \n","                                   cjs = self.color_jitter_value,\n","                                   m_obj_cov = self.min_obj_cov_value,\n","                                   a_range = (self.min_obj_cov_value, 1.0))\n","            \n","            return (view1, view2)\n","        \n","        except Exception as err:\n","            return err\n","    \n","    def get_byol_dataset(self, batch_size, dataset_type=\"train\"):\n","        \"\"\"\n","            this method, will gives the byol dataset, which is nothing but a tf.data.Dataset object.\n","            Params:\n","                batch_size(dtype: int)    : Batch Size.\n","                dataset_type(dtype: str)  : which type of dataset needed, (train, test or val)\n","                \n","            return(type: tf.data.Dataset)\n","                returns the tf.data.Dataset for intended dataset_type, by preprocessing and converting \n","                the np data.\n","        \"\"\"\n","        try:\n","            if dataset_type == \"train\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__train_X))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .map(self.__preprocess_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","            \n","            if dataset_type == \"test\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__test_X))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .map(self.__preprocess_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","            \n","            if dataset_type == \"val\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__val_X))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .map(self.__preprocess_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","        \n","        except Exception as err:\n","            return err\n","    \n","    def get_downstream_data(self): \n","        \"\"\"\n","            this method returns the dataset for the downstream task.\n","        \"\"\"\n","        return (self.__train_X, self.__train_y)#, (self.__val_X, self.__val_y), (self.__test_X, self.__test_y)\n","    \n","    def __get_valdata(self, nval):\n","        \"\"\"\n","            this method is used to create a validation data by randomly sampling from the testing data.\n","            Params:\n","                nval(dtype: Int); Number of validation data needed, rest of test_X.shape[0] - nval, will be \n","                                  testing data size.\n","            returns(type; np.ndarray, np.ndarray):\n","                returns the testing and validation dataset.\n","        \"\"\"\n","        try: \n","            ind_arr = np.arange(10_000)\n","            val_inds = np.random.choice(ind_arr, nval)\n","            test_inds = [i for i in ind_arr if not i in val_inds]\n","\n","            self.__test_X, self.__test_y = self.__dtest_X[test_inds], self.__dtest_y[test_inds]\n","            self.__val_X, self.__val_y = self.__dtest_X[val_inds], self.__dtest_y[val_inds]\n","            \n","        except Exception as err:\n","            raise err    \n","        \n","    def get_downstream_tf_dataset(self, batch_size, dataset_type=\"train\"): \n","        \"\"\"\n","             this method, will gives the downstream dataset, which is of type tf.data.Dataset object.\n","            Params:\n","                batch_size(dtype: int)    : Batch Size.\n","                dataset_type(dtype: str)  : which type of dataset needed, (train, test or val)\n","                \n","            return(type: tf.data.Dataset)\n","                returns the tf.data.Dataset for intended dataset_type, by preprocessing and converting \n","                the np data.\n","        \"\"\"\n","        assert dataset_type in [\"train\", \"test\", \"val\"], \"Given dataset type is not valid\"\n","        try:\n","            if dataset_type == \"train\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__train_X, self.__train_y))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","            \n","            if dataset_type == \"test\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__test_X, self.__test_X))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","            \n","            if dataset_type == \"val\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__val_X, self.__val_y))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","        \n","        except Exception as err:\n","            return err"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T11:00:07.431337Z","iopub.status.busy":"2023-05-09T11:00:07.430991Z","iopub.status.idle":"2023-05-09T11:00:07.444597Z","shell.execute_reply":"2023-05-09T11:00:07.443762Z","shell.execute_reply.started":"2023-05-09T11:00:07.431281Z"},"trusted":true},"outputs":[],"source":["class ByolAugmentor: \n","    \"\"\"\n","        This class is used for the data augmentation for the byol model.\n","        Methods: \n","            __random_crop_flip_resize(scope: private)\n","            __random_color_distortion(scope: private)\n","            augment(scope: public)\n","    \"\"\"\n","    def __init__(self): \n","        pass \n","    \n","    @tf.function\n","    def __random_crop_flip_resize(self, image, resize_shape):\n","        \"\"\"\n","            this method does a random crop with height and width of the crop are sampled randomly. it does the \n","            crop with the height and width, then it does a resizing again to the original shape. And also it does\n","            a flip of the image\n","            Params:\n","                image(type: tf.Tensor)   : image data of type tensor.\n","                resize_shape(type: int)  : Size of the image.\n","            Return(type: tf.Tensor)\n","                returns the crop and resized image.\n","        \"\"\"\n","        try: \n","            h_crop = tf.cast(tf.random.uniform(shape=[], minval=13, maxval=33, dtype=tf.int32), tf.float32)\n","            w_crop = h_crop * tf.random.uniform(shape=[], minval=0.67, maxval=1.0)\n","            h_crop, w_crop = tf.cast(h_crop, tf.int32), tf.cast(w_crop, tf.int32)\n","            opposite_aspectratio = tf.random.uniform(shape=[])\n","            if opposite_aspectratio < 0.5:\n","                h_crop, w_crop = w_crop, h_crop\n","            image = tf.image.random_crop(image, size=[h_crop, w_crop, 3])\n","\n","            horizontal_flip = tf.random.uniform(shape=[])\n","            if horizontal_flip < 0.5:\n","                image = tf.image.random_flip_left_right(image)\n","\n","            image = tf.image.resize(image, size=[resize_shape, resize_shape])\n","            return image\n","        \n","        except Exception as err:\n","            return err\n","    \n","    @tf.function\n","    def __random_color_distortion(self, image):\n","        \"\"\"\n","            this method, will do the color disortion augmentation for the given image.\n","            Params:\n","                image(type: tf.Tensor)   : image data of type tensor.\n","            Return(type: tf.Tensor)\n","                returns the crop and resized image.\n","        \"\"\"\n","        try: \n","            color_jitter = tf.random.uniform(shape=[])\n","            if color_jitter < 0.8:\n","                image = tf.image.random_brightness(image, max_delta=0.4)\n","                image = tf.image.random_contrast(image, lower=0.6, upper=1.4)\n","                image = tf.image.random_saturation(image, lower=0.6, upper=1.4)\n","                image = tf.image.random_hue(image, max_delta=0.1)\n","                image = tf.clip_by_value(image, 0, 1)\n","\n","            color_drop = tf.random.uniform(shape=[])\n","            if color_drop < 0.2:\n","                image = tf.image.rgb_to_grayscale(image)\n","                image = tf.tile(image, [1, 1, 3])\n","\n","            return image\n","        \n","        except Exception as error:\n","            return error\n","    \n","    @tf.function\n","    def augment(self, image, resize_shape): \n","        \"\"\"\n","            this method will include all the augmentation as a pipeline(random crop, random flip, resize, and \n","            color disortion), this augment method will be used by DataLoader class.\n","            Params:\n","                image(type: tf.Tensor)   : image data of type tensor.\n","                resize_shape(type: int)  : Size of the image.\n","            Return(type: tf.Tensor)\n","                returns the crop and resized image.\n","                \n","        \"\"\"\n","        try: \n","            image = self.__random_crop_flip_resize(image, resize_shape)\n","          #  image = self.__random_color_distortion(image)\n","\n","            return image\n","        \n","        except Exception as error:\n","            print(error, error)\n","            return error"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T13:00:49.592927Z","iopub.status.busy":"2023-05-09T13:00:49.592554Z","iopub.status.idle":"2023-05-09T13:00:50.507422Z","shell.execute_reply":"2023-05-09T13:00:50.506471Z","shell.execute_reply.started":"2023-05-09T13:00:49.592896Z"},"trusted":true},"outputs":[],"source":["downstream_dataloader = DataLoader(\"cifar10\", downstream_data=True)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T13:00:50.509493Z","iopub.status.busy":"2023-05-09T13:00:50.509128Z","iopub.status.idle":"2023-05-09T13:00:51.764267Z","shell.execute_reply":"2023-05-09T13:00:51.763318Z","shell.execute_reply.started":"2023-05-09T13:00:50.509461Z"},"trusted":true},"outputs":[],"source":["train_ds = downstream_dataloader.get_downstream_tf_dataset(128, \"train\")"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T13:00:51.766112Z","iopub.status.busy":"2023-05-09T13:00:51.765532Z","iopub.status.idle":"2023-05-09T13:00:51.838120Z","shell.execute_reply":"2023-05-09T13:00:51.837160Z","shell.execute_reply.started":"2023-05-09T13:00:51.766076Z"},"trusted":true},"outputs":[],"source":["val_ds = downstream_dataloader.get_downstream_tf_dataset(128, \"val\")"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T13:02:09.878249Z","iopub.status.busy":"2023-05-09T13:02:09.877400Z","iopub.status.idle":"2023-05-09T13:02:10.319340Z","shell.execute_reply":"2023-05-09T13:02:10.318452Z","shell.execute_reply.started":"2023-05-09T13:02:09.878219Z"},"trusted":true},"outputs":[],"source":["trained_model.trainable = True\n","model = keras.models.Sequential(\n","    [\n","        Input(shape=(32, 32, 3), name=\"Encoder Input\"),\n","        trained_model.layers[1],\n","        trained_model.layers[2],\n","        trained_model.layers[3],\n","        Dense(10, activation='softmax')\n","    ]\n",")"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T13:02:10.321344Z","iopub.status.busy":"2023-05-09T13:02:10.321026Z","iopub.status.idle":"2023-05-09T13:02:10.326395Z","shell.execute_reply":"2023-05-09T13:02:10.325537Z","shell.execute_reply.started":"2023-05-09T13:02:10.321313Z"},"trusted":true},"outputs":[],"source":["def get_linear_model(): \n","    inputs = Input(shape=(32, 32, 3), name=\"Encoder Input\")\n","    encoder = trained_model(inputs)\n","    \n","    out = Dense(10, activation=\"softmax\")(encoder)\n","    \n","    return keras.models.Model(inputs, outputs)"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T13:02:10.328531Z","iopub.status.busy":"2023-05-09T13:02:10.327606Z","iopub.status.idle":"2023-05-09T13:02:10.364926Z","shell.execute_reply":"2023-05-09T13:02:10.364029Z","shell.execute_reply.started":"2023-05-09T13:02:10.328500Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50 (Functional)       (None, 1, 1, 2048)        23587712  \n","                                                                 \n"," gmp1 (GlobalMaxPooling2D)   (None, 2048)              0         \n","                                                                 \n"," Projection (Functional)     (None, 256)               1182464   \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 24,772,746\n","Trainable params: 24,718,602\n","Non-trainable params: 54,144\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T13:02:10.587497Z","iopub.status.busy":"2023-05-09T13:02:10.586846Z","iopub.status.idle":"2023-05-09T13:02:10.602756Z","shell.execute_reply":"2023-05-09T13:02:10.601915Z","shell.execute_reply.started":"2023-05-09T13:02:10.587466Z"},"trusted":true},"outputs":[],"source":["model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=\"acc\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.fit(train_ds, validation_data=val_ds, epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
